{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating model predictions for five days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbanalysis.classes import router,time_tabler_refac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "routes = json.loads(open('/home/student/dbanalysis/dbanalysis/resources/trimmed_routes.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not broken 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/dbanalysis/dbanalysis/classes/router.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  d[str(stopA)]['actualtime_arr_from'] = arrs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somewhat broken 1 0\n",
      "ok 1 1\n",
      "ok 1 2\n",
      "ok 1 3\n",
      "somewhat broken 1 4\n",
      "5\n",
      "broken router 1 5\n",
      "not broken 102\n",
      "somewhat broken 102 0\n",
      "somewhat broken 102 1\n",
      "ok 102 2\n",
      "not broken 104\n",
      "ok 104 0\n",
      "somewhat broken 104 1\n",
      "not broken 11\n",
      "somewhat broken 11 0\n",
      "ok 11 1\n",
      "somewhat broken 11 2\n",
      "ok 11 3\n",
      "not broken 111\n",
      "ok 111 0\n",
      "somewhat broken 111 1\n",
      "somewhat broken 111 2\n",
      "not broken 114\n",
      "somewhat broken 114 0\n",
      "somewhat broken 114 1\n",
      "not broken 116\n",
      "ok 116 0\n",
      "ok 116 1\n",
      "not broken 118\n",
      "somewhat broken 118 0\n",
      "not broken 120\n",
      "somewhat broken 120 0\n",
      "somewhat broken 120 1\n",
      "somewhat broken 120 2\n",
      "ok 120 3\n",
      "not broken 122\n",
      "cannot reindex from a duplicate axis\n",
      "broken router 122 0\n",
      "somewhat broken 122 1\n",
      "somewhat broken 122 2\n",
      "somewhat broken 122 3\n",
      "not broken 123\n",
      "somewhat broken 123 0\n",
      "somewhat broken 123 1\n",
      "somewhat broken 123 2\n",
      "somewhat broken 123 3\n",
      "not broken 13\n",
      "somewhat broken 13 0\n",
      "somewhat broken 13 1\n",
      "ok 13 2\n",
      "ok 13 3\n",
      "ok 13 4\n",
      "ok 13 5\n",
      "somewhat broken 13 6\n",
      "ok 13 7\n",
      "ok 13 8\n",
      "ok 13 9\n",
      "ok 13 10\n",
      "ok 13 11\n",
      "ok 13 12\n",
      "somewhat broken 13 13\n",
      "ok 13 14\n",
      "ok 13 15\n",
      "ok 13 16\n",
      "ok 13 17\n",
      "not broken 130\n",
      "somewhat broken 130 0\n",
      "somewhat broken 130 1\n",
      "not broken 14\n",
      "ok 14 0\n",
      "somewhat broken 14 1\n",
      "somewhat broken 14 2\n",
      "not broken 140\n",
      "[Errno 2] No such file or directory: '/data/BRM/140_0/model.bin'\n",
      "broken router 140 0\n",
      "somewhat broken 140 1\n",
      "ok 140 2\n",
      "somewhat broken 140 3\n",
      "4\n",
      "broken router 140 4\n",
      "not broken 142\n",
      "somewhat broken 142 0\n",
      "somewhat broken 142 1\n",
      "ok 142 2\n",
      "somewhat broken 142 3\n",
      "not broken 145\n",
      "ok 145 0\n",
      "somewhat broken 145 1\n",
      "ok 145 2\n",
      "ok 145 3\n",
      "somewhat broken 145 4\n",
      "ok 145 5\n",
      "ok 145 6\n",
      "ok 145 7\n",
      "somewhat broken 145 8\n",
      "somewhat broken 145 9\n",
      "somewhat broken 145 10\n",
      "somewhat broken 145 11\n",
      "somewhat broken 145 12\n",
      "ok 145 13\n",
      "somewhat broken 145 14\n",
      "not broken 14C\n",
      "ok 14C 0\n",
      "somewhat broken 14C 1\n",
      "not broken 15\n",
      "ok 15 0\n",
      "severely broken 15 1 9\n",
      "severely broken 15 2 7\n",
      "ok 15 3\n",
      "not broken 150\n",
      "somewhat broken 150 0\n",
      "ok 150 1\n",
      "somewhat broken 150 2\n",
      "not broken 151\n",
      "somewhat broken 151 0\n",
      "somewhat broken 151 1\n",
      "somewhat broken 151 2\n",
      "ok 151 3\n",
      "not broken 15A\n",
      "somewhat broken 15A 0\n",
      "somewhat broken 15A 1\n",
      "not broken 15B\n",
      "somewhat broken 15B 0\n",
      "somewhat broken 15B 1\n",
      "not broken 15D\n",
      "[Errno 2] No such file or directory: '/data/BRM/15D_0/model.bin'\n",
      "broken router 15D 0\n",
      "[Errno 2] No such file or directory: '/data/BRM/15D_1/model.bin'\n",
      "broken router 15D 1\n",
      "not broken 16\n",
      "severely broken 16 0 8\n",
      "ok 16 1\n",
      "somewhat broken 16 2\n",
      "somewhat broken 16 3\n",
      "severely broken 16 4 4\n",
      "not broken 161\n",
      "somewhat broken 161 0\n",
      "ok 161 1\n",
      "somewhat broken 161 2\n",
      "not broken 16C\n",
      "somewhat broken 16C 0\n",
      "somewhat broken 16C 1\n",
      "somewhat broken 16C 2\n",
      "ok 16C 3\n",
      "not broken 17\n",
      "somewhat broken 17 0\n",
      "ok 17 1\n",
      "ok 17 2\n",
      "ok 17 3\n",
      "ok 17 4\n",
      "ok 17 5\n",
      "6\n",
      "broken router 17 6\n",
      "[Errno 2] No such file or directory: '/data/BRM/17_7/model.bin'\n",
      "broken router 17 7\n",
      "ok 17 8\n",
      "not broken 17A\n",
      "ok 17A 0\n",
      "somewhat broken 17A 1\n",
      "ok 17A 2\n",
      "somewhat broken 17A 3\n",
      "broken 18\n",
      "not broken 184\n",
      "somewhat broken 184 0\n",
      "somewhat broken 184 1\n",
      "not broken 185\n",
      "somewhat broken 185 0\n",
      "somewhat broken 185 1\n",
      "severely broken 185 2 7\n",
      "ok 185 3\n",
      "somewhat broken 185 4\n",
      "severely broken 185 5 2\n",
      "ok 185 6\n",
      "ok 185 7\n",
      "somewhat broken 185 8\n",
      "not broken 220\n",
      "ok 220 0\n",
      "ok 220 1\n",
      "somewhat broken 220 2\n",
      "somewhat broken 220 3\n",
      "not broken 236\n",
      "somewhat broken 236 0\n",
      "somewhat broken 236 1\n",
      "not broken 238\n",
      "somewhat broken 238 0\n",
      "ok 238 1\n",
      "somewhat broken 238 2\n",
      "not broken 239\n",
      "somewhat broken 239 0\n",
      "somewhat broken 239 1\n",
      "not broken 25\n",
      "severely broken 25 0 3\n",
      "somewhat broken 25 1\n",
      "not broken 25A\n",
      "somewhat broken 25A 0\n",
      "ok 25A 1\n",
      "not broken 25B\n",
      "somewhat broken 25B 0\n",
      "somewhat broken 25B 1\n",
      "somewhat broken 25B 2\n",
      "not broken 25D\n",
      "ok 25D 0\n",
      "ok 25D 1\n",
      "not broken 25X\n",
      "somewhat broken 25X 0\n",
      "ok 25X 1\n",
      "somewhat broken 25X 2\n",
      "not broken 26\n",
      "somewhat broken 26 0\n",
      "somewhat broken 26 1\n",
      "somewhat broken 26 2\n",
      "not broken 27\n",
      "ok 27 0\n",
      "somewhat broken 27 1\n",
      "somewhat broken 27 2\n",
      "somewhat broken 27 3\n",
      "somewhat broken 27 4\n",
      "ok 27 5\n",
      "not broken 270\n",
      "somewhat broken 270 0\n",
      "somewhat broken 270 1\n",
      "not broken 27A\n",
      "ok 27A 0\n",
      "somewhat broken 27A 1\n",
      "not broken 27B\n",
      "somewhat broken 27B 0\n",
      "ok 27B 1\n",
      "ok 27B 2\n",
      "ok 27B 3\n",
      "somewhat broken 27B 4\n",
      "somewhat broken 27B 5\n",
      "ok 27B 6\n",
      "ok 27B 7\n",
      "somewhat broken 27B 8\n",
      "ok 27B 9\n",
      "somewhat broken 27B 10\n",
      "not broken 27X\n",
      "somewhat broken 27X 0\n",
      "ok 27X 1\n",
      "not broken 29A\n",
      "ok 29A 0\n",
      "somewhat broken 29A 1\n",
      "not broken 31\n",
      "somewhat broken 31 0\n",
      "somewhat broken 31 1\n",
      "somewhat broken 31 2\n",
      "somewhat broken 31 3\n",
      "not broken 31A\n",
      "ok 31A 0\n",
      "somewhat broken 31A 1\n",
      "not broken 31B\n",
      "somewhat broken 31B 0\n",
      "ok 31B 1\n",
      "somewhat broken 31B 2\n",
      "not broken 31D\n",
      "ok 31D 0\n",
      "somewhat broken 31D 1\n",
      "not broken 32\n",
      "somewhat broken 32 0\n",
      "somewhat broken 32 1\n",
      "not broken 32X\n",
      "somewhat broken 32X 0\n",
      "somewhat broken 32X 1\n",
      "not broken 33\n",
      "somewhat broken 33 0\n",
      "somewhat broken 33 1\n",
      "ok 33 2\n",
      "ok 33 3\n",
      "ok 33 4\n",
      "ok 33 5\n",
      "not broken 33A\n",
      "ok 33A 0\n",
      "ok 33A 1\n",
      "somewhat broken 33A 2\n",
      "ok 33A 3\n",
      "ok 33A 4\n",
      "not broken 33B\n",
      "somewhat broken 33B 0\n",
      "ok 33B 1\n",
      "somewhat broken 33B 2\n",
      "somewhat broken 33B 3\n",
      "not broken 33D\n",
      "[Errno 2] No such file or directory: '/data/BRM/33D_0/model.bin'\n",
      "broken router 33D 0\n",
      "[Errno 2] No such file or directory: '/data/BRM/33D_1/model.bin'\n",
      "broken router 33D 1\n",
      "not broken 33X\n",
      "ok 33X 0\n",
      "ok 33X 1\n",
      "ok 33X 2\n",
      "not broken 37\n",
      "somewhat broken 37 0\n",
      "somewhat broken 37 1\n",
      "ok 37 2\n",
      "not broken 38\n",
      "somewhat broken 38 0\n",
      "somewhat broken 38 1\n",
      "somewhat broken 38 2\n",
      "somewhat broken 38 3\n",
      "somewhat broken 38 4\n",
      "not broken 38A\n",
      "ok 38A 0\n",
      "somewhat broken 38A 1\n",
      "somewhat broken 38A 2\n",
      "somewhat broken 38A 3\n",
      "not broken 38B\n",
      "ok 38B 0\n",
      "somewhat broken 38B 1\n",
      "somewhat broken 38B 2\n",
      "not broken 38D\n",
      "ok 38D 0\n",
      "ok 38D 1\n",
      "not broken 39\n",
      "somewhat broken 39 0\n",
      "somewhat broken 39 1\n",
      "somewhat broken 39 2\n",
      "somewhat broken 39 3\n",
      "not broken 39A\n",
      "severely broken 39A 0 5\n",
      "severely broken 39A 1 5\n",
      "somewhat broken 39A 2\n",
      "ok 39A 3\n",
      "ok 39A 4\n",
      "somewhat broken 39A 5\n",
      "not broken 39X\n",
      "[Errno 2] No such file or directory: '/data/BRM/39X_0/model.bin'\n",
      "broken router 39X 0\n",
      "[Errno 2] No such file or directory: '/data/BRM/39X_1/model.bin'\n",
      "broken router 39X 1\n",
      "not broken 4\n",
      "ok 4 0\n",
      "ok 4 1\n",
      "somewhat broken 4 2\n",
      "somewhat broken 4 3\n",
      "not broken 40\n",
      "severely broken 40 0 22\n",
      "somewhat broken 40 1\n",
      "somewhat broken 40 2\n",
      "ok 40 3\n",
      "[Errno 2] No such file or directory: '/data/BRM/40_4/model.bin'\n",
      "broken router 40 4\n",
      "ok 40 5\n",
      "ok 40 6\n",
      "not broken 40B\n",
      "ok 40B 0\n",
      "ok 40B 1\n",
      "somewhat broken 40B 2\n",
      "not broken 40D\n",
      "somewhat broken 40D 0\n",
      "ok 40D 1\n",
      "somewhat broken 40D 2\n",
      "somewhat broken 40D 3\n",
      "ok 40D 4\n",
      "[Errno 2] No such file or directory: '/data/BRM/40D_5/model.bin'\n",
      "broken router 40D 5\n",
      "not broken 41\n",
      "ok 41 0\n",
      "somewhat broken 41 1\n",
      "ok 41 2\n",
      "somewhat broken 41 3\n",
      "somewhat broken 41 4\n",
      "somewhat broken 41 5\n",
      "ok 41 6\n",
      "ok 41 7\n",
      "8\n",
      "broken router 41 8\n",
      "9\n",
      "broken router 41 9\n",
      "not broken 41A\n",
      "somewhat broken 41A 0\n",
      "not broken 41B\n",
      "ok 41B 0\n",
      "somewhat broken 41B 1\n",
      "ok 41B 2\n",
      "not broken 41C\n",
      "ok 41C 0\n",
      "somewhat broken 41C 1\n",
      "somewhat broken 41C 2\n",
      "not broken 41X\n",
      "ok 41X 0\n",
      "somewhat broken 41X 1\n",
      "ok 41X 2\n",
      "somewhat broken 41X 3\n",
      "ok 41X 4\n",
      "ok 41X 5\n",
      "ok 41X 6\n",
      "ok 41X 7\n",
      "not broken 42\n",
      "somewhat broken 42 0\n",
      "somewhat broken 42 1\n",
      "not broken 42D\n",
      "ok 42D 0\n",
      "ok 42D 1\n",
      "not broken 43\n",
      "somewhat broken 43 0\n",
      "ok 43 1\n",
      "somewhat broken 43 2\n",
      "somewhat broken 43 3\n",
      "not broken 44\n",
      "somewhat broken 44 0\n",
      "somewhat broken 44 1\n",
      "ok 44 2\n",
      "somewhat broken 44 3\n",
      "not broken 44B\n",
      "ok 44B 0\n",
      "somewhat broken 44B 1\n",
      "not broken 45A\n",
      "ok 45A 0\n",
      "somewhat broken 45A 1\n",
      "somewhat broken 45A 2\n",
      "somewhat broken 45A 3\n",
      "somewhat broken 45A 4\n",
      "ok 45A 5\n",
      "not broken 46A\n",
      "somewhat broken 46A 0\n",
      "somewhat broken 46A 1\n",
      "ok 46A 2\n",
      "somewhat broken 46A 3\n",
      "somewhat broken 46A 4\n",
      "somewhat broken 46A 5\n",
      "ok 46A 6\n",
      "ok 46A 7\n",
      "ok 46A 8\n",
      "ok 46A 9\n",
      "ok 46A 10\n",
      "ok 46A 11\n",
      "[Errno 2] No such file or directory: '/data/BRM/46A_12/model.bin'\n",
      "broken router 46A 12\n",
      "ok 46A 13\n",
      "not broken 46E\n",
      "somewhat broken 46E 0\n",
      "broken 47\n",
      "not broken 49\n",
      "somewhat broken 49 0\n",
      "ok 49 1\n",
      "[Errno 2] No such file or directory: '/data/BRM/49_2/model.bin'\n",
      "broken router 49 2\n",
      "not broken 51D\n",
      "ok 51D 0\n",
      "somewhat broken 51D 1\n",
      "not broken 51X\n",
      "ok 51X 0\n",
      "not broken 53\n",
      "ok 53 0\n",
      "ok 53 1\n",
      "not broken 54A\n",
      "somewhat broken 54A 0\n",
      "somewhat broken 54A 1\n",
      "not broken 56A\n",
      "somewhat broken 56A 0\n",
      "ok 56A 1\n",
      "not broken 59\n",
      "somewhat broken 59 0\n",
      "ok 59 1\n",
      "not broken 61\n",
      "ok 61 0\n",
      "somewhat broken 61 1\n",
      "somewhat broken 61 2\n",
      "somewhat broken 61 3\n",
      "not broken 63\n",
      "ok 63 0\n",
      "ok 63 1\n",
      "somewhat broken 63 2\n",
      "somewhat broken 63 3\n",
      "ok 63 4\n",
      "somewhat broken 63 5\n",
      "6\n",
      "broken router 63 6\n",
      "not broken 65\n",
      "somewhat broken 65 0\n",
      "somewhat broken 65 1\n",
      "ok 65 2\n",
      "somewhat broken 65 3\n",
      "somewhat broken 65 4\n",
      "somewhat broken 65 5\n",
      "not broken 65B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somewhat broken 65B 0\n",
      "somewhat broken 65B 1\n",
      "not broken 66\n",
      "somewhat broken 66 0\n",
      "ok 66 1\n",
      "somewhat broken 66 2\n",
      "somewhat broken 66 3\n",
      "not broken 66A\n",
      "ok 66A 0\n",
      "somewhat broken 66A 1\n",
      "not broken 66B\n",
      "somewhat broken 66B 0\n",
      "somewhat broken 66B 1\n",
      "not broken 66X\n",
      "somewhat broken 66X 0\n",
      "ok 66X 1\n",
      "somewhat broken 66X 2\n",
      "somewhat broken 66X 3\n",
      "somewhat broken 66X 4\n",
      "somewhat broken 66X 5\n",
      "somewhat broken 66X 6\n",
      "somewhat broken 66X 7\n",
      "ok 66X 8\n",
      "not broken 67\n",
      "somewhat broken 67 0\n",
      "ok 67 1\n",
      "not broken 67X\n",
      "somewhat broken 67X 0\n",
      "somewhat broken 67X 1\n",
      "somewhat broken 67X 2\n",
      "ok 67X 3\n",
      "somewhat broken 67X 4\n",
      "broken 68\n",
      "broken 68A\n",
      "broken 68X\n",
      "broken 69\n",
      "broken 69X\n",
      "not broken 7\n",
      "somewhat broken 7 0\n",
      "somewhat broken 7 1\n",
      "ok 7 2\n",
      "ok 7 3\n",
      "not broken 70\n",
      "somewhat broken 70 0\n",
      "severely broken 70 1 11\n",
      "ok 70 2\n",
      "not broken 70D\n",
      "ok 70D 0\n",
      "somewhat broken 70D 1\n",
      "not broken 747\n",
      "[Errno 2] No such file or directory: '/data/BRM/747_0/model.bin'\n",
      "broken router 747 0\n",
      "[Errno 2] No such file or directory: '/data/BRM/747_1/model.bin'\n",
      "broken router 747 1\n",
      "[Errno 2] No such file or directory: '/data/BRM/747_2/model.bin'\n",
      "broken router 747 2\n",
      "not broken 75\n",
      "somewhat broken 75 0\n",
      "somewhat broken 75 1\n",
      "somewhat broken 75 2\n",
      "ok 75 3\n",
      "somewhat broken 75 4\n",
      "not broken 757\n",
      "somewhat broken 757 0\n",
      "somewhat broken 757 1\n",
      "not broken 76\n",
      "ok 76 0\n",
      "somewhat broken 76 1\n",
      "not broken 76A\n",
      "somewhat broken 76A 0\n",
      "ok 76A 1\n",
      "not broken 77A\n",
      "somewhat broken 77A 0\n",
      "somewhat broken 77A 1\n",
      "not broken 77X\n",
      "somewhat broken 77X 0\n",
      "not broken 79\n",
      "somewhat broken 79 0\n",
      "somewhat broken 79 1\n",
      "not broken 79A\n",
      "somewhat broken 79A 0\n",
      "ok 79A 1\n",
      "not broken 7A\n",
      "somewhat broken 7A 0\n",
      "somewhat broken 7A 1\n",
      "somewhat broken 7A 2\n",
      "somewhat broken 7A 3\n",
      "not broken 7B\n",
      "ok 7B 0\n",
      "somewhat broken 7B 1\n",
      "broken 7D\n",
      "not broken 83\n",
      "somewhat broken 83 0\n",
      "ok 83 1\n",
      "ok 83 2\n",
      "ok 83 3\n",
      "ok 83 4\n",
      "ok 83 5\n",
      "ok 83 6\n",
      "ok 83 7\n",
      "not broken 83A\n",
      "somewhat broken 83A 0\n",
      "ok 83A 1\n",
      "ok 83A 2\n",
      "somewhat broken 83A 3\n",
      "not broken 84\n",
      "somewhat broken 84 0\n",
      "somewhat broken 84 1\n",
      "ok 84 2\n",
      "ok 84 3\n",
      "somewhat broken 84 4\n",
      "ok 84 5\n",
      "not broken 84A\n",
      "somewhat broken 84A 0\n",
      "somewhat broken 84A 1\n",
      "somewhat broken 84A 2\n",
      "ok 84A 3\n",
      "not broken 84X\n",
      "ok 84X 0\n",
      "somewhat broken 84X 1\n",
      "ok 84X 2\n",
      "[Errno 2] No such file or directory: '/data/BRM/84X_3/model.bin'\n",
      "broken router 84X 3\n",
      "somewhat broken 84X 4\n",
      "somewhat broken 84X 5\n",
      "somewhat broken 84X 6\n",
      "not broken 9\n",
      "somewhat broken 9 0\n",
      "ok 9 1\n",
      "somewhat broken 9 2\n",
      "somewhat broken 9 3\n"
     ]
    }
   ],
   "source": [
    "t=time_tabler_refac.time_tabler()\n",
    "totally_broken_routes = []\n",
    "severely_broken_routes = []\n",
    "somewhat_broken_routes = []\n",
    "broken_routers = []\n",
    "ok_routes = []\n",
    "for route in routes:\n",
    "    import datetime\n",
    "    dt = datetime.datetime.now()\n",
    "    try:\n",
    "        \n",
    "        a=t.get_dep_times_five_days(route,dt)\n",
    "        print('not broken',route)\n",
    "    except:\n",
    "        totally_broken_routes.append(route)\n",
    "        print('broken',route)\n",
    "        continue\n",
    "    for v_num, variation in enumerate(routes[route]):\n",
    "        try:\n",
    "            r=router.router(route,v_num,variation)\n",
    "            df=r.generate_all_times(a[v_num]['matrix'])\n",
    "            total = 0\n",
    "            for key in df:\n",
    "                d=df[key]\n",
    "                thing= d[d['actualtime_arr_to']<d['actualtime_arr_from']].shape[0]\n",
    "                if thing > 0:\n",
    "                    total += 1\n",
    "            if total == 1:\n",
    "                somewhat_broken_routes.append([route,v_num,total])\n",
    "                print('somewhat broken',route,v_num)\n",
    "                    \n",
    "                    \n",
    "            elif total > 1:\n",
    "                severely_broken_routes.append([route,v_num,total])\n",
    "                print('severely broken',route,v_num,total)\n",
    "                    \n",
    "            else:\n",
    "                ok_routes.append(['ok',route,v_num])\n",
    "                print('ok',route,v_num)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            broken_routers.append([route,v_num])\n",
    "            print('broken router',route,v_num)\n",
    "            pass\n",
    "import pickle\n",
    "with open('/data/broken_models.bin','wb') as handle:\n",
    "    obj = {'totally broken':totally_broken_routes,\\\n",
    "          'severely broken':severely_broken_routes,\\\n",
    "          'somewhat broken':somewhat_broken_routes,\\\n",
    "          'broken routers':broken_routers,\\\n",
    "          'ok routes':ok_routes}\n",
    "    pickle.dump(obj,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r15 = routes['46A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(routes['46A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=router.router('46A',0,r15)\n",
    "t=time_tabler_refac.time_tabler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.now()\n",
    "d=t.get_dep_times_five_days('46A',dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/dbanalysis/dbanalysis/classes/router.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  d[str(stopA)]['actualtime_arr_from'] = arrs\n"
     ]
    }
   ],
   "source": [
    "df=r.generate_all_times(d[0]['matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n"
     ]
    }
   ],
   "source": [
    "for key in df:\n",
    "    d=df[key]\n",
    "    print(d[d['actualtime_arr_to']<d['actualtime_arr_from']].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are a whole bunch or routes where the times are coming out negative.\n",
    "\n",
    "# Whatever this is, its not science\n",
    "\n",
    "\n",
    "\n",
    "Two things here - a) the model has obviously never seen data for July so maybe we should drop month as a feature.\n",
    "\n",
    "b) The hour for these faulty predictions is either 6 or 23.\n",
    "\n",
    "Lets try this on some more routes\n",
    "\n",
    "## Changing the variation keys around, it seems the error might be something to do with like... some kind of mix up in the keys supplied to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/dbanalysis/dbanalysis/classes/router.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  d[str(stopA)]['actualtime_arr_from'] = arrs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    actualtime_arr_from  day  busID  month  weekend  variation routeid  hour  \\\n",
      "0                 25800    0      0      8    False          0     46A     7   \n",
      "1                 28500    0      1      8    False          0     46A     7   \n",
      "2                 26700    0      2      8    False          0     46A     7   \n",
      "3                 27600    0      3      8    False          0     46A     7   \n",
      "4                 28500    0      4      8    False          0     46A     7   \n",
      "5                 25800    1      0      8    False          0     46A     7   \n",
      "6                 28500    1      1      8    False          0     46A     7   \n",
      "7                 26700    1      2      8    False          0     46A     7   \n",
      "8                 27600    1      3      8    False          0     46A     7   \n",
      "9                 28500    1      4      8    False          0     46A     7   \n",
      "10                25800    2      0      8    False          0     46A     7   \n",
      "11                28500    2      1      8    False          0     46A     7   \n",
      "12                26700    2      2      8    False          0     46A     7   \n",
      "13                27600    2      3      8    False          0     46A     7   \n",
      "14                28500    2      4      8    False          0     46A     7   \n",
      "15                25800    3      0      8    False          0     46A     7   \n",
      "16                28500    3      1      8    False          0     46A     7   \n",
      "17                26700    3      2      8    False          0     46A     7   \n",
      "18                27600    3      3      8    False          0     46A     7   \n",
      "19                28500    3      4      8    False          0     46A     7   \n",
      "\n",
      "    distance stopA stopB  actualtime_arr_to  \n",
      "0   0.400321  2060  2061       25406.286151  \n",
      "1   0.400321  2060  2061       28106.286151  \n",
      "2   0.400321  2060  2061       26306.286151  \n",
      "3   0.400321  2060  2061       27206.286151  \n",
      "4   0.400321  2060  2061       28106.286151  \n",
      "5   0.400321  2060  2061       25620.083735  \n",
      "6   0.400321  2060  2061       28320.083735  \n",
      "7   0.400321  2060  2061       26520.083735  \n",
      "8   0.400321  2060  2061       27420.083735  \n",
      "9   0.400321  2060  2061       28320.083735  \n",
      "10  0.400321  2060  2061       25356.052611  \n",
      "11  0.400321  2060  2061       28056.052611  \n",
      "12  0.400321  2060  2061       26256.052611  \n",
      "13  0.400321  2060  2061       27156.052611  \n",
      "14  0.400321  2060  2061       28056.052611  \n",
      "15  0.400321  2060  2061       25497.360204  \n",
      "16  0.400321  2060  2061       28197.360204  \n",
      "17  0.400321  2060  2061       26397.360204  \n",
      "18  0.400321  2060  2061       27297.360204  \n",
      "19  0.400321  2060  2061       28197.360204  \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [actualtime_arr_from, day, busID, month, weekend, variation, routeid, hour, distance, stopA, stopB, actualtime_arr_to]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4b1c31ad6f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actualtime_arr_to'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actualtime_arr_from'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r15 = routes['46A'][0]\n",
    "r=router.router('46A',0,r15)\n",
    "t=time_tabler_refac.time_tabler()\n",
    "import datetime\n",
    "dt = datetime.datetime.now()\n",
    "d=t.get_dep_times_five_days('46A',dt)\n",
    "df=r.generate_all_times(d[0]['matrix'])\n",
    "for key in df:\n",
    "    d=df[key]\n",
    "    print(d[d['actualtime_arr_to']<d['actualtime_arr_from']])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well here its just for the first stop, so its not a massive problem.\n",
    "I would be inerested though if dropping the month feature would change this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-94-b2f5d5be43a9>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-b2f5d5be43a9>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    elif rgr == 'Neural':\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from dbanalysis import stop_tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class BRModel():\n",
    "    \"\"\"\n",
    "    Big route model class\n",
    "    uses the distance of a stop from first stop on a route to compute predictions lalalala\n",
    "    MAPE and r2 scores are not as good as they were in the notebook\n",
    "    (we achieved 0.57 r2, and 7% MAPE on the time to complete the route)\n",
    "    Should look into this.\n",
    "    Planned time of arrival boosts score considerably\n",
    "    \"\"\"\n",
    "    def __init__ (self, route,variation,verbose=True,src='build',rgr='RandomForest',\\\n",
    "                mode='validate',features = ['distance','vappr'],use_dummies=True):\n",
    "        \n",
    "        import json\n",
    "        self.regr_type = rgr\n",
    "        self.verbose = verbose\n",
    "        self.route = route\n",
    "        self.use_dummies = use_dummies\n",
    "        self.variation = variation\n",
    "        self.routes = json.loads(open('/home/student/dbanalysis/dbanalysis/resources/trimmed_routes.json').read())\n",
    "        self.features = features\n",
    "        \n",
    "        self.route_array = self.routes[route][variation][1:]\n",
    "        \n",
    "        if src == 'build':\n",
    "            if not self.can_be_modelled():\n",
    "                print('fuck')\n",
    "                raise ValueError ('Missing data for modelling this route')\n",
    "\n",
    "            self.gather_data()\n",
    "            self.preprocess()\n",
    "            if rgr == 'RandomForest':\n",
    "                from sklearn.ensemble import RandomForestRegressor as rf\n",
    "                self.rgr = rf()\n",
    "            elif rgr == 'Linear':\n",
    "                from sklearn.linear_model import LinearRegression as lr\n",
    "                self.rgr = lr(fit_intercept=True)\n",
    "            elif rgr == 'Neural':\n",
    "                from sklearn.neural_network import MLPRegressor as mlpr\n",
    "                self.rgr = mlpr(hidden_layer_sizes=(100,),\n",
    "                                activation='tanh')\n",
    "                del(self.routes)\n",
    "                if mode == 'validate':\n",
    "                    self.validate_neural_network()\n",
    "                else:\n",
    "                    self.build_neural_network()\n",
    "    \n",
    "    \n",
    "    def build_neural_network(self):\n",
    "        import numpy as np\n",
    "        msk = np.random.rand(len(self.data)) < 0.5\n",
    "        self.train = self.data[msk]\n",
    "        del(msk)\n",
    "        \n",
    "        from sklearn.preprocessing import MinMaxScaler as mms\n",
    "        del(self.data)\n",
    "        self.X_transformer = mms().fit(self.train[self.features])\n",
    "        Y = self.train['traveltime'].values\n",
    "        Y=Y.reshape(-1,1)\n",
    "        self.Y_transformer = mms().fit(Y)\n",
    "        X = self.X_transformer.transform(self.train[self.features])\n",
    "        Y = self.Y_transformer.transform(Y)\n",
    "        \n",
    "        self.model = self.rgr.fit(X,Y)\n",
    "        print('Built')\n",
    "        del(X)\n",
    "        del(Y)\n",
    "        del(self.train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def validate_neural_network(self):\n",
    "        \n",
    "        self.train = self.data[self.data['year']==2016]\n",
    "        self.test = self.data[self.data['year'] == 2017]\n",
    "        from sklearn.preprocessing import MinMaxScaler as mms\n",
    "        del(self.data)\n",
    "        self.X_transformer = mms().fit(self.train[self.features])\n",
    "        Y = self.train['traveltime'].values\n",
    "        Y=Y.reshape(-1,1)\n",
    "        self.Y_transformer = mms().fit(Y)\n",
    "        X = self.X_transformer.transform(self.train[self.features])\n",
    "        Y = self.Y_transformer.transform(Y)\n",
    "        del(self.train)\n",
    "        self.model = self.rgr.fit(X,Y)\n",
    "        del(X)\n",
    "        del(Y)\n",
    "        distances = sorted(self.test['distance'].unique())[1:]\n",
    "        number_samples = []\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        mape = []\n",
    "        from sklearn import metrics\n",
    "        for i in range(0, len(distances)-1):\n",
    "            test = self.test[(self.test['distance']>=distances[i]) & (self.test['distance'] < distances[i+1])]\n",
    "            Y = test['traveltime']\n",
    "            number_samples.append(len(test))\n",
    "            X = self.X_transformer.transform(test[self.features])\n",
    "            preds = self.model.predict(X)\n",
    "            real_preds = self.Y_transformer.inverse_transform(preds.reshape(-1,1))\n",
    "            real_preds = np.array([i[0] for i in real_preds])\n",
    "            print(real_preds.mean())\n",
    "            input()\n",
    "            r2_score = metrics.r2_score(Y, real_preds)\n",
    "            MAE = metrics.mean_absolute_error(Y,real_preds)\n",
    "            MAPE = ((abs(Y - real_preds)/Y)*100).mean()\n",
    "            r2.append(r2_score)\n",
    "            mae.append(MAE)\n",
    "            mape.append(MAPE)\n",
    "            print(r2_score,MAE,MAPE)\n",
    "        self.distances = distances[:-1]\n",
    "        del(self.test)\n",
    "        del(test)\n",
    "        del(preds)        \n",
    "    def build_model():\n",
    "        import numpy as np\n",
    "        if self.verbose:\n",
    "            print('building model')\n",
    "        #how many rows are we going to train on. Here we just randomly pick 50% of the samples and use that. Full df is probably too large\n",
    "        msk = np.random.rand(len(self.data)) < 0.5\n",
    "        self.data = self.data[msk]\n",
    "        del(msk)\n",
    "        self.model = self.rgr.predict(self.data[features].values,self.data[msk].values)\n",
    "        del(self.data)\n",
    "    def dump_model(self):\n",
    "        import pickle\n",
    "        import time\n",
    "        t = int(time.time())\n",
    "        name = str(self.route) + '_' + str(self.variation) + '_' + str(time.time()) +'.bin'\n",
    "        with open('/data/BRModels/models/'+name,'wb') as handle:\n",
    "            pickle.dump(self,handle,protocol = pickle.HIGHEST_PROTOCOL) \n",
    "        print('Saved Model')\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "    def gather_data(self):\n",
    "        if self.verbose:\n",
    "            print('gathering data...')\n",
    "        from dbanalysis import stop_tools\n",
    "        arr = self.route_array\n",
    "        import os\n",
    "        to_concat = []\n",
    "        for i in range(len(arr)-1):\n",
    "            \n",
    "            data = stop_tools.get_stop_link(arr[i],arr[i+1])\n",
    "            if data is not None:\n",
    "                \n",
    "            \n",
    "                routeids = data['routeid'].unique()\n",
    "                valid_routeids = [r for r in routeids if r.split('_')[0] == self.route]\n",
    "                data = data[data['routeid'].isin(valid_routeids)]\n",
    "\n",
    "                to_concat.append(data)\n",
    "            \n",
    "            del(data)\n",
    "        self.data = pd.concat(to_concat,axis=0)\n",
    "        del to_concat\n",
    "        \n",
    "\n",
    "    def preprocess(self):\n",
    "        if self.verbose:\n",
    "            print('Preprocessing data')\n",
    "        self.select_routes()\n",
    "        self.clean_1()\n",
    "        self.add_distances()\n",
    "        \n",
    "        self.add_base_departure_time()\n",
    "        self.add_time_info()\n",
    "        self.merge_weather()\n",
    "        if self.use_dummies:\n",
    "            self.add_dummies()\n",
    "            self.features += self.dummy_features\n",
    "        rubbish=self.data[self.data['traveltime']<0]\n",
    "        self.data['planned_traveltime'] = self.data['plannedtime_arr_from'] - self.data['base_time_dep']\n",
    "    def select_routes(self):\n",
    "        if self.verbose:\n",
    "            print('parsing routeids')\n",
    "        routeids = self.data['routeid'].unique()\n",
    "        valid_routeids = [r for r in routeids if r.split('_')[0] == self.route]\n",
    "        self.data = self.data[self.data['routeid'].isin(valid_routeids)]\n",
    "    def clean_1(self):\n",
    "        if self.verbose:\n",
    "            print('dropping null values')\n",
    "        self.data = self.data.dropna()\n",
    "        \n",
    "    def add_distances(self):\n",
    "        if self.verbose:\n",
    "            print('adding distances')\n",
    "        s_getter =stop_tools.stop_getter()\n",
    "        total_distance = 0\n",
    "        r = self.route_array\n",
    "        route_distances = {r[0]:0}\n",
    "        \n",
    "        for i in range(0, len(r)-1):\n",
    "            distance = s_getter.get_stop_distance(str(r[i]),str(r[i+1]))\n",
    "                \n",
    "            total_distance += distance\n",
    "            route_distances[r[i+1]]=total_distance\n",
    "        self.data['distance']=self.data['stopA'].apply(lambda x: route_distances[x])\n",
    "        del(s_getter)\n",
    "    def add_base_departure_time(self):\n",
    "        if self.verbose:\n",
    "            print('adding base departure times')\n",
    "       \n",
    "        keys= self.data[self.data['stopA']==self.route_array[0]]\n",
    "        keys['base_time_dep']=keys['actualtime_dep_from']\n",
    "        keys2=keys[['tripid','dayofservice','base_time_dep']]\n",
    "        self.data = pd.merge(self.data,keys2,on=['dayofservice','tripid'])\n",
    "        \n",
    "        self.data['traveltime']=self.data['actualtime_arr_from']-self.data['base_time_dep']        \n",
    "        del(keys)\n",
    "        del(keys2)\n",
    "    \n",
    "    def add_time_info(self):\n",
    "        if self.verbose:\n",
    "            print('adding time information')\n",
    "        time_format = \"%d-%b-%y %H:%M:%S\"\n",
    "        self.data['dt']=pd.to_datetime(self.data['dayofservice'],format=time_format)\n",
    "        self.data['day']=self.data['dt'].dt.dayofweek\n",
    "        self.data['month']=self.data['dt'].dt.month\n",
    "        self.data['hour'] = self.data['base_time_dep'] //3600\n",
    "        self.data['weekend']=self.data['day']>4\n",
    "        self.data['year']=self.data['dt'].dt.year\n",
    "        self.data['date'] = self.data['dt'].dt.date\n",
    "    def merge_weather(self,weather=None):\n",
    "        if self.verbose:\n",
    "            print('merging weather')\n",
    "        if weather == None:\n",
    "          \n",
    "            weather = pd.read_csv('/home/student/dbanalysis/dbanalysis/resources/cleanweather.csv').dropna()\n",
    "        weather['dt']=pd.to_datetime(weather['date'])\n",
    "        weather['hour']=weather['dt'].dt.hour\n",
    "        weather['date']=weather['dt'].dt.date\n",
    "        \n",
    "        self.data = pd.merge(self.data,weather,on=['date','hour'])\n",
    "        del(weather)\n",
    "\n",
    "    def add_dummies(self):\n",
    "        if self.verbose:\n",
    "            print('Making dummy features')\n",
    "        self.data = pd.get_dummies(self.data,columns=['day','hour'])\n",
    "        for hour in range(5,25):\n",
    "            if 'hour_'+str(hour) not in self.data.columns:\n",
    "                self.data['hour_'+str(hour)] = 0\n",
    "\n",
    "        self.dummy_features = [col for col in self.data.columns\\\n",
    "                                if (col[0:3] == 'day' and col != 'dayofservice')\\\n",
    "                                or col[0:5] == 'month' or col[0:4] == 'hour']\n",
    "       \n",
    "    def can_be_modelled(self):\n",
    "        if self.verbose:\n",
    "            print('Checking for data files')\n",
    "        import os\n",
    "        base_dir = '/data/stops/'\n",
    "        arr = self.route_array\n",
    "        #if the first stop is missing, then we won't attempt to model this route\n",
    "            \n",
    "        if os.path.exists(base_dir+str(arr[1])+'/'+str(arr[2])+'.csv'):\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model without month as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n",
      "gathering data...\n",
      "Preprocessing data\n",
      "parsing routeids\n",
      "dropping null values\n",
      "adding distances\n",
      "adding base departure times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:210: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding time information\n",
      "merging weather\n",
      "Making dummy features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built\n"
     ]
    }
   ],
   "source": [
    "r=BRModel('15',1,mode='build',rgr='Neural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=t.get_dep_times_five_days('15',dt)\n",
    "df = d[1]['matrix']\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(df,columns=['day','hour'])\n",
    "df['vappr']=10.0\n",
    "for f in r.features:\n",
    "    if f not in df.columns:\n",
    "        df[f]=0\n",
    "X = r.X_transformer.transform(df[r.features])\n",
    "Y=r.model.predict(X)\n",
    "a = r.Y_transformer.inverse_transform(Y.reshape(-1,1))\n",
    "count1=0\n",
    "count2=0\n",
    "for i in a:\n",
    "    if i > 0:\n",
    "        count1+=1\n",
    "    else:\n",
    "        count2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d[1]['matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.get_dummies(df,columns=['day','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vappr']=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in r.features:\n",
    "    if f not in df.columns:\n",
    "        df[f]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = r.X_transformer.transform(df[r.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=r.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = r.Y_transformer.inverse_transform(Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1=0\n",
    "count2=0\n",
    "for i in a:\n",
    "    if i > 0:\n",
    "        count1+=1\n",
    "    else:\n",
    "        count2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30388"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than 1 % of predictions are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different activation functions above - led to more negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `results` not found.\n"
     ]
    }
   ],
   "source": [
    "Does a linear regression model give negative results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does a linear regression model give negative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbanalysis import stop_tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class BRModel():\n",
    "    \"\"\"\n",
    "    Big route model class\n",
    "    uses the distance of a stop from first stop on a route to compute predictions lalalala\n",
    "    MAPE and r2 scores are not as good as they were in the notebook\n",
    "    (we achieved 0.57 r2, and 7% MAPE on the time to complete the route)\n",
    "    Should look into this.\n",
    "    Planned time of arrival boosts score considerably\n",
    "    \"\"\"\n",
    "    def __init__ (self, route,variation,verbose=True,src='build',rgr='RandomForest',\\\n",
    "                mode='validate',features = ['distance','vappr'],use_dummies=True):\n",
    "        \n",
    "        import json\n",
    "        self.regr_type = rgr\n",
    "        self.verbose = verbose\n",
    "        self.route = route\n",
    "        self.use_dummies = use_dummies\n",
    "        self.variation = variation\n",
    "        self.routes = json.loads(open('/home/student/dbanalysis/dbanalysis/resources/trimmed_routes.json').read())\n",
    "        self.features = features\n",
    "        \n",
    "        self.route_array = self.routes[route][variation][1:]\n",
    "        \n",
    "        if src == 'build':\n",
    "            if not self.can_be_modelled():\n",
    "                print('fuck')\n",
    "                raise ValueError ('Missing data for modelling this route')\n",
    "\n",
    "            self.gather_data()\n",
    "            self.preprocess()\n",
    "            if rgr == 'RandomForest':\n",
    "                from sklearn.ensemble import RandomForestRegressor as rf\n",
    "                self.rgr = rf()\n",
    "            elif rgr == 'Linear':\n",
    "                from sklearn.linear_model import LinearRegression as lr\n",
    "                self.rgr = lr(fit_intercept=True)\n",
    "                \n",
    "            elif rgr == 'Neural':\n",
    "                from sklearn.linear_model import LinearRegression as lr\n",
    "                self.rgr = lr(fit_intercept=True)\n",
    "                del(self.routes)\n",
    "                if mode == 'validate':\n",
    "                    self.validate_neural_network()\n",
    "                else:\n",
    "                    self.build_neural_network()\n",
    "    \n",
    "    \n",
    "    def build_neural_network(self):\n",
    "        import numpy as np\n",
    "        msk = np.random.rand(len(self.data)) < 0.5\n",
    "        self.train = self.data[msk]\n",
    "        del(msk)\n",
    "        \n",
    "        from sklearn.preprocessing import MinMaxScaler as mms\n",
    "        del(self.data)\n",
    "        self.X_transformer = mms().fit(self.train[self.features])\n",
    "        Y = self.train['traveltime'].values\n",
    "        Y=Y.reshape(-1,1)\n",
    "        self.Y_transformer = mms().fit(Y)\n",
    "        X = self.X_transformer.transform(self.train[self.features])\n",
    "        Y = self.Y_transformer.transform(Y)\n",
    "        \n",
    "        self.model = self.rgr.fit(X,Y)\n",
    "        print('Built')\n",
    "        del(X)\n",
    "        del(Y)\n",
    "        del(self.train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def validate_neural_network(self):\n",
    "        \n",
    "        self.train = self.data[self.data['year']==2016]\n",
    "        self.test = self.data[self.data['year'] == 2017]\n",
    "        from sklearn.preprocessing import MinMaxScaler as mms\n",
    "        del(self.data)\n",
    "        self.X_transformer = mms().fit(self.train[self.features])\n",
    "        Y = self.train['traveltime'].values\n",
    "        Y=Y.reshape(-1,1)\n",
    "        self.Y_transformer = mms().fit(Y)\n",
    "        X = self.X_transformer.transform(self.train[self.features])\n",
    "        Y = self.Y_transformer.transform(Y)\n",
    "        del(self.train)\n",
    "        self.model = self.rgr.fit(X,Y)\n",
    "        del(X)\n",
    "        del(Y)\n",
    "        distances = sorted(self.test['distance'].unique())[1:]\n",
    "        number_samples = []\n",
    "        r2 = []\n",
    "        mae = []\n",
    "        mape = []\n",
    "        from sklearn import metrics\n",
    "        for i in range(0, len(distances)-1):\n",
    "            test = self.test[(self.test['distance']>=distances[i]) & (self.test['distance'] < distances[i+1])]\n",
    "            Y = test['traveltime']\n",
    "            number_samples.append(len(test))\n",
    "            X = self.X_transformer.transform(test[self.features])\n",
    "            preds = self.model.predict(X)\n",
    "            real_preds = self.Y_transformer.inverse_transform(preds.reshape(-1,1))\n",
    "            real_preds = np.array([i[0] for i in real_preds])\n",
    "            print(real_preds.mean())\n",
    "            input()\n",
    "            r2_score = metrics.r2_score(Y, real_preds)\n",
    "            MAE = metrics.mean_absolute_error(Y,real_preds)\n",
    "            MAPE = ((abs(Y - real_preds)/Y)*100).mean()\n",
    "            r2.append(r2_score)\n",
    "            mae.append(MAE)\n",
    "            mape.append(MAPE)\n",
    "            print(r2_score,MAE,MAPE)\n",
    "        self.distances = distances[:-1]\n",
    "        del(self.test)\n",
    "        del(test)\n",
    "        del(preds)        \n",
    "    def build_model():\n",
    "        import numpy as np\n",
    "        if self.verbose:\n",
    "            print('building model')\n",
    "        #how many rows are we going to train on. Here we just randomly pick 50% of the samples and use that. Full df is probably too large\n",
    "        msk = np.random.rand(len(self.data)) < 0.5\n",
    "        self.data = self.data[msk]\n",
    "        del(msk)\n",
    "        self.model = self.rgr.predict(self.data[features].values,self.data[msk].values)\n",
    "        del(self.data)\n",
    "    def dump_model(self):\n",
    "        import pickle\n",
    "        import time\n",
    "        t = int(time.time())\n",
    "        name = str(self.route) + '_' + str(self.variation) + '_' + str(time.time()) +'.bin'\n",
    "        with open('/data/BRModels/models/'+name,'wb') as handle:\n",
    "            pickle.dump(self,handle,protocol = pickle.HIGHEST_PROTOCOL) \n",
    "        print('Saved Model')\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "    def gather_data(self):\n",
    "        if self.verbose:\n",
    "            print('gathering data...')\n",
    "        from dbanalysis import stop_tools\n",
    "        arr = self.route_array\n",
    "        import os\n",
    "        to_concat = []\n",
    "        for i in range(len(arr)-1):\n",
    "            \n",
    "            data = stop_tools.get_stop_link(arr[i],arr[i+1])\n",
    "            if data is not None:\n",
    "                \n",
    "            \n",
    "                routeids = data['routeid'].unique()\n",
    "                valid_routeids = [r for r in routeids if r.split('_')[0] == self.route]\n",
    "                data = data[data['routeid'].isin(valid_routeids)]\n",
    "\n",
    "                to_concat.append(data)\n",
    "            \n",
    "            del(data)\n",
    "        self.data = pd.concat(to_concat,axis=0)\n",
    "        del to_concat\n",
    "        \n",
    "\n",
    "    def preprocess(self):\n",
    "        if self.verbose:\n",
    "            print('Preprocessing data')\n",
    "        self.select_routes()\n",
    "        self.clean_1()\n",
    "        self.add_distances()\n",
    "        \n",
    "        self.add_base_departure_time()\n",
    "        self.add_time_info()\n",
    "        rubbish = self.data[self.data['traveltime']<0]\n",
    "        self.data=self.data[~self.data[['dayofservice','tripid']].isin(rubbish[['dayofservice','tripid']])]\n",
    "        self.merge_weather()\n",
    "        if self.use_dummies:\n",
    "            self.add_dummies()\n",
    "            self.features += self.dummy_features\n",
    "        self.data=self.data[self.data['traveltime']>0]\n",
    "        self.data['planned_traveltime'] = self.data['plannedtime_arr_from'] - self.data['base_time_dep']\n",
    "    def select_routes(self):\n",
    "        if self.verbose:\n",
    "            print('parsing routeids')\n",
    "        routeids = self.data['routeid'].unique()\n",
    "        valid_routeids = [r for r in routeids if r.split('_')[0] == self.route]\n",
    "        self.data = self.data[self.data['routeid'].isin(valid_routeids)]\n",
    "    def clean_1(self):\n",
    "        if self.verbose:\n",
    "            print('dropping null values')\n",
    "        self.data = self.data.dropna()\n",
    "        \n",
    "    def add_distances(self):\n",
    "        if self.verbose:\n",
    "            print('adding distances')\n",
    "        s_getter =stop_tools.stop_getter()\n",
    "        total_distance = 0\n",
    "        r = self.route_array\n",
    "        route_distances = {r[0]:0}\n",
    "        \n",
    "        for i in range(0, len(r)-1):\n",
    "            distance = s_getter.get_stop_distance(str(r[i]),str(r[i+1]))\n",
    "                \n",
    "            total_distance += distance\n",
    "            route_distances[r[i+1]]=total_distance\n",
    "        self.data['distance']=self.data['stopA'].apply(lambda x: route_distances[x])\n",
    "        del(s_getter)\n",
    "    def add_base_departure_time(self):\n",
    "        if self.verbose:\n",
    "            print('adding base departure times')\n",
    "       \n",
    "        keys= self.data[self.data['stopA']==self.route_array[0]]\n",
    "        keys['base_time_dep']=keys['actualtime_dep_from']\n",
    "        keys2=keys[['tripid','dayofservice','base_time_dep']]\n",
    "        self.data = pd.merge(self.data,keys2,on=['dayofservice','tripid'])\n",
    "        \n",
    "        self.data['traveltime']=self.data['actualtime_arr_from']-self.data['base_time_dep']        \n",
    "        del(keys)\n",
    "        del(keys2)\n",
    "    \n",
    "    def add_time_info(self):\n",
    "        if self.verbose:\n",
    "            print('adding time information')\n",
    "        time_format = \"%d-%b-%y %H:%M:%S\"\n",
    "        self.data['dt']=pd.to_datetime(self.data['dayofservice'],format=time_format)\n",
    "        self.data['day']=self.data['dt'].dt.dayofweek\n",
    "        self.data['month']=self.data['dt'].dt.month\n",
    "        self.data['hour'] = self.data['base_time_dep'] //3600\n",
    "        self.data['weekend']=self.data['day']>4\n",
    "        self.data['year']=self.data['dt'].dt.year\n",
    "        self.data['date'] = self.data['dt'].dt.date\n",
    "    def merge_weather(self,weather=None):\n",
    "        if self.verbose:\n",
    "            print('merging weather')\n",
    "        if weather == None:\n",
    "          \n",
    "            weather = pd.read_csv('/home/student/dbanalysis/dbanalysis/resources/cleanweather.csv').dropna()\n",
    "        weather['dt']=pd.to_datetime(weather['date'])\n",
    "        weather['hour']=weather['dt'].dt.hour\n",
    "        weather['date']=weather['dt'].dt.date\n",
    "        \n",
    "        self.data = pd.merge(self.data,weather,on=['date','hour'])\n",
    "        del(weather)\n",
    "\n",
    "    def add_dummies(self):\n",
    "        if self.verbose:\n",
    "            print('Making dummy features')\n",
    "        self.data = pd.get_dummies(self.data,columns=['day','hour'])\n",
    "        for hour in range(5,25):\n",
    "            if 'hour_'+str(hour) not in self.data.columns:\n",
    "                self.data['hour_'+str(hour)] = 0\n",
    "\n",
    "        self.dummy_features = [col for col in self.data.columns\\\n",
    "                                if (col[0:3] == 'day' and col != 'dayofservice')\\\n",
    "                                or col[0:5] == 'month' or col[0:4] == 'hour']\n",
    "       \n",
    "    def can_be_modelled(self):\n",
    "        if self.verbose:\n",
    "            print('Checking for data files')\n",
    "        import os\n",
    "        base_dir = '/data/stops/'\n",
    "        arr = self.route_array\n",
    "        #if the first stop is missing, then we won't attempt to model this route\n",
    "            \n",
    "        if os.path.exists(base_dir+str(arr[1])+'/'+str(arr[2])+'.csv'):\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=BRModel('15',1,mode='just data',rgr='Neural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for data files\n",
      "gathering data...\n",
      "Preprocessing data\n",
      "parsing routeids\n",
      "dropping null values\n",
      "adding distances\n",
      "adding base departure times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:210: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding time information\n",
      "merging weather\n",
      "Making dummy features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built\n",
      "[-381.5446434]\n",
      "\n",
      "[-282.75359726]\n",
      "\n",
      "[-282.75359726]\n",
      "\n",
      "[-282.75359726]\n",
      "\n",
      "[-281.12963486]\n",
      "\n",
      "[-281.12963486]\n",
      "\n",
      "[-264.07802963]\n",
      "\n",
      "[-281.12963486]\n",
      "\n",
      "[-264.07802963]\n",
      "\n",
      "[-264.07802963]\n",
      "\n",
      "[-375.86077499]\n",
      "\n",
      "[-375.86077499]\n",
      "\n",
      "[-432.63179398]\n",
      "\n",
      "[-375.86077499]\n",
      "\n",
      "[-432.63179398]\n",
      "\n",
      "[-576.14947128]\n",
      "\n",
      "[-576.14947128]\n",
      "\n",
      "\n",
      "[-383.98058701]\n",
      "[-383.98058701]\n",
      "\n",
      "[-400.89686203]\n",
      "\n",
      "[-400.89686203]\n",
      "\n",
      "[-381.5446434]\n",
      "\n",
      "[-400.89686203]\n",
      "\n",
      "[-381.5446434]\n",
      "\n",
      "[-59.39110184]\n",
      "\n",
      "[-94.91527939]\n",
      "\n",
      "[-94.91527939]\n",
      "\n",
      "[-94.91527939]\n",
      "\n",
      "[-383.98058701]\n",
      "\n",
      "[-155.54320908]\n",
      "\n",
      "[-155.54320908]\n",
      "\n",
      "[-155.54320908]\n",
      "\n",
      "[-59.39110184]\n",
      "\n",
      "[-59.39110184]\n",
      "\n",
      "[-898.77666855]\n",
      "\n",
      "[-783.47533798]\n",
      "\n",
      "[-898.77666855]\n",
      "\n",
      "[-753.97335434]\n",
      "\n",
      "[-783.47533798]\n",
      "\n",
      "[-101.0051384]\n",
      "\n",
      "[-101.0051384]\n",
      "\n",
      "[-644.62655258]\n",
      "\n",
      "[-644.62655258]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-202.36745834]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-93.83263779]\n",
      "\n",
      "[-202.36745834]\n",
      "\n",
      "[-202.36745834]\n",
      "\n",
      "[-202.36745834]\n",
      "\n",
      "[-202.36745834]\n"
     ]
    }
   ],
   "source": [
    "r=BRModel('15',1,mode='build',rgr='Neural')\n",
    "d=t.get_dep_times_five_days('15',dt)\n",
    "df = d[1]['matrix']\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(df,columns=['day','hour'])\n",
    "df['vappr']=10.0\n",
    "for f in r.features:\n",
    "    if f not in df.columns:\n",
    "        df[f]=0\n",
    "X = r.X_transformer.transform(df[r.features])\n",
    "Y=r.model.predict(X)\n",
    "a = r.Y_transformer.inverse_transform(Y.reshape(-1,1))\n",
    "count1=0\n",
    "count2=0\n",
    "for i in a:\n",
    "    if i > 0:\n",
    "        count1+=1\n",
    "    else:\n",
    "        print(i)\n",
    "        input()\n",
    "        count2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30613"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still negative even with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, for now at least, this will have to be solved on the technical, non modelling level. We will explicitly use the mean predicted value for the first few stops or something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok so solution to the problem. For the first stops on the route, reset the time of arrival at this stop to equal the time of arrival at the next stop, + some number imputed from the distance. \n",
    "\n",
    "# Unsure what to do about the routes where there are negative travel times for multiple stops.\n",
    "\n",
    "# Though we could, in theory, just not model these routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rubbish' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-701a791a3933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrubbish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rubbish' is not defined"
     ]
    }
   ],
   "source": [
    "rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
